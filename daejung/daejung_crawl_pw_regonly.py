# -*- coding: utf-8 -*-
"""
ëŒ€ì • ê²€ìƒ‰ ìµœì†Œ ë°ì´í„° ìˆ˜ì§‘ (ê°€ì„±ë¹„ & ì†ë„ ìµœì í™”íŒ - labels ê°œì„  í’€ë²„ì „)
- ë¸Œë¼ìš°ì € 1íšŒë§Œ ë„ìš°ê³  ì¬ì‚¬ìš© (ì‹±ê¸€í†¤)
- ì´ë¯¸ì§€/í°íŠ¸/ë¯¸ë””ì–´ ì°¨ë‹¨(ìŠ¤í¬ë¦½íŠ¸ëŠ” í—ˆìš©) â†’ ë™ì  ë Œë”ë§ ìœ ì§€
- ì²« ê²°ê³¼ë§Œ/ë¼ë²¨ í¬í•¨ ì—¬ë¶€ ì˜µì…˜
- âœ… íŒì—… í´ë¦­ X â†’ onclick/href/í–‰ ì „ì²´ì—ì„œ idx ì¶”ì¶œ í›„ popup URLë¡œ ì§ì ‘ ì ‘ì†
- âœ… íŒì—…ì´ iframe/ë™ì ì‚½ì…ì´ì–´ë„ íŒŒì‹±ë˜ë„ë¡ 'í”„ë ˆì„ ìˆœíšŒ + ë¡œë”© ëŒ€ê¸°' ê°•í™”
- âœ… ì‹¤íŒ¨ ì‹œ 1íšŒ ë””ë²„ê·¸ ë¤í”„(HTML, í”„ë ˆì„ ë³¸ë¬¸)ë¡œ ì›ì¸ ê³ ì •
"""

from playwright.sync_api import sync_playwright
from decimal import Decimal, ROUND_HALF_UP
import re, json, time, urllib.request, os
from typing import Optional

# -------------------- ìƒìˆ˜ --------------------
BASE = "https://www.daejungchem.co.kr"
SEARCH_URL = f"{BASE}/02_product/search/"
HEADLESS = True

DEFAULT_TIMEOUT = 15000   # ìš”ì†Œ ëŒ€ê¸°(ms)
GOTO_TIMEOUT = 25000      # í˜ì´ì§€ ì§„ì…(ms)

LAUNCH_ARGS = [
    "--no-sandbox",
    "--disable-dev-shm-usage",
    "--disable-blink-features=AutomationControlled",
]

_rx_int = re.compile(r"(\d[\d,]*)")

# ---- table indices (0-based) ----
TD_IDX = {
    "cas": 1,        # 1-based 2
    "code": 2,       # 1-based 3
    "name": 3,       # 1-based 4
    "pack": 5,       # 1-based 6
    "price": 7,      # 1-based 8
    "stock": 8,      # 1-based 9
}

# -------------------- ì‹±ê¸€í†¤ ë¸Œë¼ìš°ì € --------------------
_play = None
_browser = None

def get_browser():
    global _play, _browser
    if _play is None:
        _play = sync_playwright().start()
    if _browser is None:
        _browser = _play.chromium.launch(headless=HEADLESS, args=LAUNCH_ARGS)
    return _browser

def stop_browser():
    global _play, _browser
    try:
        if _browser: _browser.close()
    except Exception:
        pass
    try:
        if _play: _play.stop()
    except Exception:
        pass
    _browser = None; _play = None

# -------------------- ìœ í‹¸ --------------------
def ping():
    try:
        with urllib.request.urlopen(SEARCH_URL, timeout=8) as r:
            return r.status
    except Exception as e:
        return f"ERR:{e}"

def parse_int(s: str) -> Optional[int]:
    m = _rx_int.search(s or "")
    return int(m.group(1).replace(",", "")) if m else None

def discount_round(price: Optional[int], rate: float = 0.10, unit: int = 100) -> Optional[int]:
    if price is None:
        return None
    val = Decimal(price) * Decimal(1 - rate)
    return int((val / unit).quantize(Decimal("1"), rounding=ROUND_HALF_UP) * unit)

def safe_text(loc, fallback=""):
    try:
        return loc.inner_text().strip()
    except Exception:
        return fallback

def find_search_input(page):
    for sel in (
        "form input[type='search']","form input[type='text']",
        "input[type='search']","input[type='text']",
        "input[placeholder*='ê²€ìƒ‰']","input[placeholder*='search' i]"
    ):
        loc = page.locator(sel)
        if loc.count():
            return loc.first
    raise RuntimeError("ê²€ìƒ‰ ì…ë ¥ì°½ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# -------------------- idx ì¶”ì¶œ (í–‰ ì „ì²´ ì»¤ë²„) --------------------
IDX_RXES = [
    re.compile(r"idx\s*=\s*([A-Za-z0-9_-]+)"),
    re.compile(r"popup[^)]*?\bidx[^A-Za-z0-9_-]*?([A-Za-z0-9_-]+)"),
    re.compile(r"\bfn_\w+\s*\(\s*([A-Za-z0-9_-]+)\s*\)"),
    re.compile(r"data-idx\s*=\s*['\"]?([A-Za-z0-9_-]+)['\"]?"),
    re.compile(r"/popup/\?idx=([A-Za-z0-9_-]+)"),
]

def extract_idx_from_row(row) -> Optional[str]:
    # ì œí’ˆëª… ì•µì»¤ ìš°ì„ 
    try:
        a = row.locator("td").nth(TD_IDX['name']).locator("a")
        onclick = a.first.get_attribute("onclick") if a.count() else ""
        href    = a.first.get_attribute("href")    if a.count() else ""
    except Exception:
        onclick = ""; href = ""

    s = f"{onclick or ''} {href or ''}"

    # í–‰ ì „ì²´ outerHTMLê¹Œì§€ í•©ì³ íŒ¨í„´ ê²€ìƒ‰
    try:
        outer = row.evaluate("el => el.outerHTML")
        s += " " + (outer or "")
    except Exception:
        pass

    for rx in IDX_RXES:
        m = rx.search(s)
        if m:
            return m.group(1)
    return None

# -------------------- íŒì—… íŒŒì‹± ë³´ì¡° --------------------
def extract_texts_from_page_or_frames(p):
    """ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ë©”ì¸ body + ëª¨ë“  iframeì˜ bodyì—ì„œ ìˆ˜ì§‘"""
    texts = []
    # 1) ë©”ì¸
    try:
        body = p.inner_text("body")
        if body and body.strip():
            texts.append(body)
    except Exception:
        pass
    # 2) í”„ë ˆì„
    try:
        for fr in p.frames:
            if fr == p.main_frame:
                continue
            try:
                b = fr.inner_text("body")
                if b and b.strip():
                    texts.append(b)
            except Exception:
                continue
    except Exception:
        pass
    return texts

def extract_regulation_lines(page):
    """íŒì—…ì—ì„œ ê·œì œì •ë³´ ê´€ë ¨ ì¤„ë§Œ ì¶”ì¶œ (í‘œ ìš°ì„  â†’ ë³¸ë¬¸/í”„ë ˆì„ ë³´ì¡°)"""
    lines = []
    # í‘œ ìš°ì„  íŒŒì‹±
    try:
        tds = page.locator("table td, .popup table td, .table td, .layer table td")
        if tds.count() > 0:
            for i in range(tds.count()):
                txt = (tds.nth(i).inner_text() or "").strip()
                if txt:
                    lines.append(txt)
    except Exception:
        pass

    # í‘œê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•˜ë©´ ë³¸ë¬¸/í”„ë ˆì„
    if not lines:
        try:
            blobs = extract_texts_from_page_or_frames(page)
            raw = "\n".join(blobs)
            lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
        except Exception:
            lines = []

    keep_keys = (
        "ë¬¼ì§ˆ","ë¥˜","ìœ í•´","ìœ„í—˜","ê·œì œ","Remark","ê¸°ì¡´ë¬¼ì§ˆ","ìœ í•´í™”í•™ë¬¼ì§ˆ",
        "ì‹ ê·œí™”í•™ë¬¼ì§ˆ","ê¸°ì¡´ í™”í•™ë¬¼ì§ˆ","ì‚°ì—…ì•ˆì „ë³´ê±´ë²•","í™”í•™ë¬¼ì§ˆ","ê´€ê³„ë²•ë ¹"
    )
    kept = [ln for ln in lines if any(k in ln for k in keep_keys)]

    # ì¤‘ë³µ ì œê±° + ê³µë°± ì •ë¦¬
    seen, out = set(), []
    for ln in kept:
        ln2 = re.sub(r"\s+", " ", ln)
        if ln2 not in seen:
            out.append(ln2); seen.add(ln2)
    return out

def debug_dump_popup(p, prefix="popup"):
    """ë¬¸ì œ ì›ì¸ ê³ ì •ì„ ìœ„í•œ 1íšŒ ë¤í”„"""
    try:
        os.makedirs("debug_dump", exist_ok=True)
        with open(os.path.join("debug_dump", f"{prefix}_main.html"), "w", encoding="utf-8") as f:
            f.write(p.content())
        for i, fr in enumerate(p.frames):
            try:
                b = fr.inner_text("body")
                with open(os.path.join("debug_dump", f"{prefix}_frame_{i}.txt"), "w", encoding="utf-8") as f:
                    f.write(b or "")
            except Exception:
                pass
    except Exception:
        pass

# âœ… íŒì—… í´ë¦­ ì—†ì´, idxë¥¼ ì´ìš©í•´ ì§ì ‘ íŒì—… URL ì ‘ì† â†’ ë¼ë²¨ ì¶”ì¶œ
def fetch_labels_by_idx(ctx, idx: str) -> list[str]:
    url = f"{BASE}/02_product/popup/?idx={idx}"
    p = ctx.new_page()
    try:
        p.set_default_timeout(DEFAULT_TIMEOUT)
        p.set_default_navigation_timeout(GOTO_TIMEOUT)

        # íŒì—… í˜ì´ì§€ì—ë„ ë¦¬ì†ŒìŠ¤ ë¼ìš°íŒ… ì ìš© (script í—ˆìš©)
        def _route(route):
            if route.request.resource_type in {"image","font","media"}:
                return route.abort()
            return route.continue_()
        p.route("**/*", _route)

        # loadê¹Œì§€ ëŒ€ê¸° + networkidle ë³´ì¡°
        p.goto(url, wait_until="load", timeout=GOTO_TIMEOUT)
        try:
            p.wait_for_load_state("networkidle", timeout=4000)
        except Exception:
            pass

        # ë™ì  ì‚½ì… ëŒ€ë¹„ ì§§ì€ ì¬í™•ì¸ ë£¨í”„
        for _ in range(2):
            # í‘œ/ì»¨í…Œì´ë„ˆ ì¡´ì¬ í™•ì¸ í›„ ì•½ê°„ ë” ëŒ€ê¸°
            if p.locator("table, .popup, .layer, body").count() > 0:
                try:
                    p.wait_for_load_state("networkidle", timeout=2000)
                except Exception:
                    pass
            txts = extract_texts_from_page_or_frames(p)
            if any(len(t.strip()) > 50 for t in txts):
                break
            p.wait_for_timeout(600)

        labels = extract_regulation_lines(p)
        if not labels:
            # ìµœì´ˆ ì‹¤íŒ¨ ì‹œ ë¤í”„
            debug_dump_popup(p, prefix=f"popup_idx_{idx}")
        return labels

    except Exception:
        return []
    finally:
        try:
            p.close()
        except Exception:
            pass

# -------------------- ë©”ì¸ ê²€ìƒ‰ --------------------
def search_minimal(keyword: str, first_only: bool = False, include_labels: bool = False):
    """í•„ìˆ˜ ìµœì†Œ ë°ì´í„°ë§Œ ìˆ˜ì§‘
    - first_only: ì²« í–‰ë§Œ ë°˜í™˜(ê°€ì¥ ë¹ ë¦„)
    - include_labels: í•„ìš”í•  ë•Œë§Œ ë¼ë²¨ ìˆ˜ì§‘(íŒì—… URL ì§ì ‘ ì ‘ì†)
    """
    def goto_with_retry(page, url, attempts=3):
        last_err = None
        for i in range(attempts):
            try:
                page.goto(url, wait_until="commit", timeout=GOTO_TIMEOUT)
                page.wait_for_load_state("domcontentloaded", timeout=DEFAULT_TIMEOUT)
                page.wait_for_selector(
                    "form input[type='search'], form input[type='text'], input[placeholder*='ê²€ìƒ‰'], input[placeholder*='search' i]",
                    timeout=DEFAULT_TIMEOUT
                )
                return
            except Exception as e:
                last_err = e
                page.wait_for_timeout(1200 * (i + 1))
                try:
                    page.reload(timeout=GOTO_TIMEOUT)
                except Exception:
                    pass
        raise last_err

    browser = get_browser()
    ctx = browser.new_context(
        user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                    "KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"),
        viewport={"width": 1366, "height": 900},
        locale="ko-KR",
        timezone_id="Asia/Seoul",
    )
    # ìë™í™” íƒì§€ ìš°íšŒ ì‘ì€ ë³´ì •
    ctx.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined});")

    page = ctx.new_page()
    page.set_default_timeout(DEFAULT_TIMEOUT)
    page.set_default_navigation_timeout(GOTO_TIMEOUT)

    # ë¦¬ì†ŒìŠ¤ ì ˆê°: ì´ë¯¸ì§€/í°íŠ¸/ë¯¸ë””ì–´ ì°¨ë‹¨ (script í—ˆìš©)
    def _route(route):
        if route.request.resource_type in {"image", "font", "media"}:
            return route.abort()
        return route.continue_()
    page.route("**/*", _route)

    # 1) ê²€ìƒ‰ í˜ì´ì§€ ì§„ì…
    goto_with_retry(page, SEARCH_URL, attempts=3)

    # 2) ê²€ìƒ‰ì–´ ì…ë ¥ â†’ ì—”í„°
    box = find_search_input(page)
    box.fill("")
    box.type(keyword)
    box.press("Enter")

    # 3) ê²°ê³¼ ëŒ€ê¸°(ì œì¶œ ë²„íŠ¼ ë³´ì¡°)
    try:
        page.wait_for_selector("tbody tr", timeout=DEFAULT_TIMEOUT)
    except Exception:
        btn = page.locator("form button, button[type='submit'], input[type='submit']")
        if btn.count():
            btn.first.click()
        page.wait_for_selector("tbody tr", timeout=DEFAULT_TIMEOUT)

    rows = page.locator("tbody tr")
    n = rows.count()
    if n == 0:
        ts = int(time.time())
        page.screenshot(path=f"daejung_debug_{ts}.png", full_page=True)
        with open(f"daejung_debug_{ts}.html", "w", encoding="utf-8") as f:
            f.write(page.content())
        ctx.close()
        return []

    # ì²« í–‰ ë””ë²„ê·¸(1íšŒ) - idx/ì•µì»¤ ì†ì„± í™•ì¸ ìš©
    debug_dumped_row = False

    items = []
    for i in range(n):
        tds = rows.nth(i).locator("td")
        if tds.count() <= TD_IDX["stock"]:
            continue

        code = safe_text(tds.nth(TD_IDX["code"])) or None
        price = parse_int(safe_text(tds.nth(TD_IDX["price"])))
        stock_label = safe_text(tds.nth(TD_IDX["stock"]))

        labels = []
        if include_labels and (not first_only or i == 0):
            idx = extract_idx_from_row(rows.nth(i))
            if not debug_dumped_row:
                try:
                    row_html = rows.nth(i).evaluate("el => el.outerHTML")
                    os.makedirs("debug_dump", exist_ok=True)
                    with open(os.path.join("debug_dump", "row_debug.html"), "w", encoding="utf-8") as f:
                        f.write(row_html or "")
                    a = rows.nth(i).locator("td").nth(TD_IDX['name']).locator("a")
                    if a.count():
                        with open(os.path.join("debug_dump", "a_debug.txt"), "w", encoding="utf-8") as f:
                            f.write("onclick=" + str(a.first.get_attribute("onclick")) + "\n")
                            f.write("href=" + str(a.first.get_attribute("href")) + "\n")
                except Exception:
                    pass
                debug_dumped_row = True

            labels = fetch_labels_by_idx(ctx, idx) if idx else []

        items.append({
            "brand": "ëŒ€ì •í™”ê¸ˆ",
            "code": code,
            "price": price,
            "discount_price": discount_round(price, unit=100),
            "stock_label": stock_label,
            "labels": labels,
        })

        if first_only:
            break

    ctx.close()
    return items

# -------------------- CLI ì‹¤í–‰ --------------------
if __name__ == "__main__":
    try:
        kw = input("ğŸ” ëŒ€ì • ì œí’ˆì½”ë“œ ë˜ëŠ” í‚¤ì›Œë“œ: ").strip()
    except EOFError:
        kw = ""
    data = search_minimal(kw or "ì—íƒ„ì˜¬", first_only=True, include_labels=True)
    print(json.dumps(data, ensure_ascii=False, indent=2) if data else "âŒ ê²°ê³¼ ì—†ìŒ")
